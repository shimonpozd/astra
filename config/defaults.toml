# config/defaults.toml

[llm]
provider = "openrouter"
model = "openrouter/x-ai/grok-4-fast:free"

[llm.api.openrouter]
api_key = "${OPENROUTER_API_KEY}"
base_url = "https://openrouter.ai/api/v1"
referrer = "${OPENROUTER_REFERRER:-https://astra.chat}"
title = "${OPENROUTER_TITLE:-Astra Admin}"

[llm.api.openai]
api_key = "${OPENAI_API_KEY}"
organization = "${OPENAI_ORG_ID:-}"

[llm.api.ollama]
base_url = "http://localhost:11434"

[llm.overrides]
chat = "openrouter/x-ai/grok-4-fast:free"
drafter = "openrouter/x-ai/grok-4-fast:free"
critic = "openrouter/x-ai/grok-4-fast:free"
meta_reasoner = "openrouter/x-ai/grok-4-fast:free"
curator = "openrouter/x-ai/grok-4-fast:free"
summarizer = "openrouter/x-ai/grok-4-fast:free"
translator = "openrouter/x-ai/grok-4-fast:free"
planner = "openrouter/x-ai/grok-4-fast:free"
study = "deepseek/deepseek-chat-v3.1:free"
lexicon = "openrouter/nousresearch/nous-hermes-2-mixtral-8x7b-dpo"
speechify = "openrouter/x-ai/grok-4-fast:free"

[llm.parameters]
temperature = 0.1
top_p = 0.9

[llm.tooling]
parallel_tool_calls = false
retry_on_empty_stream = true

[llm.tasks.summary]
model = "gpt-4o-mini"
temperature = 0.2
top_p = 1.0
max_tokens_out = 512
timeout_s = 25
retries = 2
backoff_ms = 400
response_format_json = true

[memory]
redis_url = "redis://localhost:6379/0"
qdrant_url = "http://localhost:6333"

[memory.embeddings]
provider = "ollama"
model = "embeddinggemma:300m"
cache_ttl_seconds = 604800

[memory.ingest]
queue_name = "astra_ltm_ingest_queue"
batch_size = 100
batch_timeout_ms = 500

[memory.features]
mask_pii = true

[memory.stm]
char_budget = 3000
min_recent = 6
buffer_size = 24
# New STM settings for brain_service
ttl_seconds = 86400
trigger_messages = 8
trigger_tokens = 2000

# Enhanced STM settings
[stm]
enabled = true
ttl_sec = 86400

[stm.trigger]
msgs_high = 10
msgs_low = 6
tokens_high = 2500
tokens_low = 1500
cooldown_sec = 30

[stm.slots]
summary_max_items = 8
facts_max_items = 50
facts_hamm_thresh = 6
open_loops_max_items = 10
refs_max_items = 10

[stm.decay]
half_life_min = 240
min_score_keep = 0.1

[stm.inject]
top_facts = 3
top_open_loops = 2
top_refs = 3
include_when_empty = false
max_chars_budget = 1200

[stm.summary]
enabled = true
input_tokens_budget = 1200
output_bullets_min = 3
output_bullets_max = 8
bullet_max_chars = 140
allow_refs = true
max_refs = 5
cooldown_sec = 30
trigger_msgs_high = 10
trigger_msgs_low = 6
trigger_tokens_high = 2500
trigger_tokens_low = 1500
log_verbose = false
partial_min_tokens = 50

[memory.ltm]
max_points = 6

[research]
default_depth = 5
max_depth = 2
curator_max_candidates = 30
note_max_chars = 3500
final_draft_max_tokens = 4000

[research.iterations]
min = 4
max = 8
base = 3
depth_divisor = 5

[export.drasha]
dir = "exports"
auto_export = true

[voice.stt]
provider = "whisper"
brain_endpoint = "http://localhost:7030/chat/voice"

[voice.stt.whisper]
model_size = "medium"
compute_type = "float16"
device = "cuda"
model_path = "models/whisper-large-v2"
forced_language = "ru"

[voice.stt.deepgram]
api_key = "${DEEPGRAM_API_KEY}"

[voice.tts]
provider = "xtts"

[voice.tts.xtts]
api_url = "http://localhost:8010"
speaker_wav = "speakers/audio.wav"

[voice.tts.elevenlabs]
api_key = "${ELEVENLABS_API_KEY}"
voice_id = "Rachel"

[voice.tts.orpheus]
api_url = "http://localhost:7041"

[voice.voice_in]
streaming_enabled = false
stt_service_url = "http://localhost:7020/stt"
stt_stream_url = "ws://localhost:7020/ws/stt"
agent_id = "default"
chunk_duration_ms = 2000
noise_reduction = false
normalization = false
adaptive_thresholds = false
auto_start = true

[voice.voice_in.interrupt]
threshold = 0.7
min_duration_ms = 300

[voice.voice_in.vad]
min_silence_ms = 800

[services]
redis_url = "redis://localhost:6379/0"
brain_url = "http://localhost:7030"
memory_service_url = "http://localhost:7050"
tts_service_url = "http://localhost:7040"
stt_service_url = "http://localhost:7020"
sefaria_mcp_url = "https://devmcp.sefaria.org/sse"
sefaria_mcp_timeout_sec = 30

[services.brain]
# Brain service specific settings
log_level = "DEBUG"
port = 7030
admin_token = "super-secret-token"
cors_origins = "http://localhost:5173,http://localhost:3000"

[services.brain.rate_limiting]
enabled = true
default_limit = 10
window_seconds = 60
llm_limit = 5

[services.brain.sefaria]
api_url = "http://localhost:8000/api/"
api_key = ""
cache_ttl_seconds = 60

[personalities]
default = "default"
path = "personalities"

[launcher.enabled_services]
voice_in = false
stt = false
brain_service = true
tts = false

[debug]
chain_of_thought = true
deep_research = true

[actions.speechify]
# Options: "english_only", "hebrew_only", "hebrew_and_english"
language_preference = "english_only"

[actions.translation]
# Setting for the explicit "Translate" button
# Options: "high" (sends Hebrew + English), "fast" (sends English only)
on_demand_quality = "high"

[actions.context]
# Setting for the context included in normal questions during Study Mode
# Options: "hebrew_and_english", "english_only", "hebrew_only"
study_mode_context = "english_only"

[study.chat_history]
# Maximum number of messages to keep in chat history
max_messages = 2000
# TTL for chat history in days
ttl_days = 30

[study]
  [study.window]
  size_min = 5
  size_default = 30
  size_max = 90

[study.daily]
  modular_loader_enabled = true
  initial_small = 120
  initial_medium = 180
  initial_large = 240
  batch_size = 120

  [study.features]
  facade_enabled = true
