# Голосовой ассистент-драшист — техническое задание

## 1. Цель и результат
- Подготовить модуль ассистента, который по запросу пользователя (пасук, недельная глава, отрывок Талмуда или тема) собирает материалы из Сефарии и проверенных справочников, индексирует их и формирует драшу в стиле сихот Любавического Ребе.
- Результат — структурированный текст с вопросом, анализом комментариев на разных уровнях (пшат, драш, каббала, алаха), сравнением мнений, практическим выводом и списком источников. Дополнительно: голосовая озвучка и поддержка длинных форматов.

## 2. Архитектура
```
Пользователь (текст/голос)
        │
        ▼
 FastAPI «Brain Service» ──► Sefaria API клиент
        │                 └─► Memory service (Qdrant + Mem0)
        │                 └─► LLM провайдеры (OpenAI/OpenRouter/Ollama)
        └─► TTS сервис (локальный XTTS/Orpheus или внешний)
        └─► Redis (кэш)
```

### 2.1 Основные компоненты
| Компонент | Назначение | Технологии |
| --- | --- | --- |
| brain-service | Маршрутизация, orchestrator, стриминг ответов LLM, tool-calling | FastAPI, httpx, OpenAI SDK, Redis |
| sefaria_client / utils / index | Работа с API Сефарии, нормализация ссылок, кэширование и алиасы | httpx, Redis |
| memory-service | Векторная память, Mem0-интеграция, Neo4j контекст | FastAPI, Qdrant, Redis, Neo4j |
| LLM-провайдеры | Генерация ответов: OpenAI, OpenRouter (DeepSeek, GPT-4, Claude), Ollama | openai, httpx |
| Vector embeddings | EmbeddingGemma (Ollama) → OpenAI embeddings (fallback) | Ollama, OpenAI |
| TTS | Локальные или внешние движки, стриминг речи | XTTS, Orpheus, aiohttp |

## 3. Функциональные требования

### 3.0 Многошаговый пайплайн deepresearch (обновление: сентябрь 2025)
1. **Планировщик (`analyze_research_request`)** — LLM распарсивает свободный запрос, возвращает focus, guiding questions, outline, списки primary/supporting refs, канонические категории и параметр `research_depth`.
2. **Сбор корпуса (`prepare_deepresearch_payload`)** — сначала скачиваем base text для каждого ref, затем лимитируем категории и комментарии по `research_depth`.
3. **Курация ссылок (`_curate_links_with_llm`)** — вторая LLM-модель выбирает до `limit` релевантных refs по категориям; при ошибках LLM работает fallback `_select_priority_links`.
4. **Чанкование и сохранение** — `chunk_text` собирает порции 200–300 символов, после чего данные идут в Qdrant/Mem0 с метаданными (`ref`, `commentator`, `category`, `role`).
5. **Подготовка финального промпта** — `_build_research_context_message` добавляет focus, outline и перечень источников в system сообщение перед генерацией drasha.

### 3.1 Входные запросы
- Примеры: `Exodus 12:2`, `Berakhot 2a`, «Спиноза и иудаизм».
- Поддерживаются текст и голос.

### 3.2 Получение источников (Сефария)
- Использовать `sefaria_get_text_v3_async` для текстов.
- Использовать `sefaria_get_links_async` для связанных материалов с фильтрацией и кэшированием.
- Категории (приоритет выборки и фильтрации):
  - Commentary — классические комментарии (Раши, Рамбам, Ибн Эзра, Сфорно и др.).
  - Quoting Commentary — комментарии, которые цитируют других авторов.
  - Midrash — мидраши.
  - Mishnah — тексты Мишны.
  - Targum — переводы (Онгелос, Йонатан и др.).
  - Halakhah — книги по еврейскому закону.
  - Responsa — שאלות ותשובות (שו"ת), вопросы и ответы.
  - Chasidut — хасидская литература (Тания, Ликутей Тора, Ликутей Сихот).
  - Kabbalah — каббала (Зоар, Шаарей Ора и пр.).
  - Jewish Thought — מחשבת ישראל (философия, еврейская мысль).
  - Liturgy — литургия (сидуры, махзоры).
  - Bible — Танах как текст (для параллелей и цитат).
  - Apocrypha — книги вне канона (например, Бен Сира).
  - Modern Works — современные исследования и книги.
- Ограничения: `limit_total = 60`, `limit_per_category = 12`, `limit_per_commentator = 3`, приоритетные комментаторы: Rashi, Ramban, Ibn Ezra, Sforno, Ralbag, Abarbanel, Bartenura. `require_leaf_for_all = True`.
- Важное: нормализация `tref`, дедупликация, сортировка, дальнейшая загрузка текстов по выбранным ссылкам.

### 3.3 Индексация в память
- Чанкование: 200–300 символов, overlap 30–50.
- Embeddings: Ollama EmbeddingGemma (622 MB, контекст 2k) → OpenAI (fallback).
- Qdrant: коллекция на каждый запрос (per-study). Метаданные: `ref`, `commentator`, `category`, `source`, `heRef`, `lang`, `collection`.

### 3.4 Генерация драш
- ReAct агент: инструменты `sefaria_get_text`, `sefaria_get_links`, `vector.search`, `memory.recall` (Mem0), wiki/chabadpedia.
- Chain-of-thought: итеративные tool-вызовы, reasoning.
- Системный промт: раввин/исследователь, стиль сихот Ребе, язык — русский, при необходимости перевод.
- Структура ответа:
  1. Введение / постановка вопроса.
  2. Анализ комментариев (пшат, драш, каббала, алаха).
  3. Сравнение мнений.
  4. Новое объяснение.
  5. Практический урок.
  6. Сноски со ссылками.

### 3.5 Длинные форматы и подкасты
- Для >5 минут: генерация плана (outline), поиск источников по разделам, chunked writing с последующим объединением.
- Поддержка 30–40 минут текста с передачей в TTS.

### 3.6 Озвучивание (второстепенный модуль)
- Текст → TTS. Варианты: локальный XTTS/Orpheus или внешние сервисы. Полная озвучка длинной драши выполняется вручную; автоматизация отложена на пост-deepresearch этап.
- Разделение на главы, опция пропуска сносок (пока опционально).

## 4. Нефункциональные требования
- Производительность: ≤5–7 c для коротких запросов, ≤20 c для длинных.
- Масштабируемость: параллельные пользователи.
- Хранение: Redis (кэш 10 мин), Qdrant (векторная память), Neo4j (граф), Mem0.
- Языки: русский (основной), поддержка иврита/арамейского в источниках.
- Безопасность: проверка цитат, минимизация галлюцинаций.

## 5. API «Brain Service»
| Метод | Путь | Назначение |
| --- | --- | --- |
| POST | `/chat/stream` | Стриминг текстового ответа |
| POST | `/chat/voice` | Ответ + TTS |
| POST | `/chat/text` | Синхронный ответ |
| POST | `/sefaria/text` | Получить текст по `ref` |
| POST | `/sefaria/links` | Получить ссылки с фильтрацией |
| POST | `/memory/store` | Передать документы в память (проксируется) |
| POST | `/memory/recall` | Поиск по памяти |
| POST | `/tts/stop` | Остановить озвучивание |
| GET | `/tts/status` | Статус TTS |

## 6. Форматы данных
```json
{
  "id": "uuid",
  "ref": "Exodus 12:2",
  "text": "Этот месяц для вас...",
  "category": "Commentary",
  "commentator": "Rashi",
  "lang": "he",
  "embedding": [ ...vector... ],
  "metadata": {
    "heRef": "שמות יב:ב",
    "source": "sefaria",
    "collection": "parasha_vaera"
  }
}
```

## 7. Дополнительные источники (Wikipedia + Хабадпедия)
### 7.1 Политика
- Приоритет: сначала RAG по Сефарии, потом справка.
- Allowlist доменов: `*.wikipedia.org`, `*.wikimedia.org`, `chabadpedia.co.il`.
- Языковой роутинг: he → en → ru для хасидских тем; иначе en → ru → he.
- Лимит: до ~8k символов внешнего текста.
- Цитирование: отдельный блок «Справочные источники» с URL.
- Кэш: Redis (TTL 6–24 ч).
- Анти-галлюцинации: справка не может опровергать канонические источники.

### 7.2 API и параметры
- Chabadpedia: `action=opensearch`, `action=parse`, `maxlag`, `User-Agent`, очистка HTML, троттлинг.
- Wikipedia: OpenAI SDK `WikipediaRetriever` или прямой MediaWiki API.
- Env:
```
WIKI_LANGS_PRIORITY=he,en,ru
WIKI_TOPK=3
WIKI_MAX_CHARS=4000
CHABADPEDIA_API=https://chabadpedia.co.il/api.php
CHABADPEDIA_TOPK=3
CHABADPEDIA_MAX_CHARS=4000
CHABADPEDIA_MAXLAG=5
WEB_UA=YourServiceName/1.0 (contact: you@example.com)
WEB_TIMEOUT_SEC=20
WEB_CACHE_TTL_SEC=21600
WEB_DOMAIN_ALLOWLIST=wikipedia.org,wikimedia.org,chabadpedia.co.il
```

### 7.3 Инструменты
- `WikiSearchTool`: OpenAI SDK retriever, top-3 результатов, 4000 символов.
- `ChabadpediaTool`: async поиск и загрузка страниц, очистка, метаданные.
- В системном промте: «Для биографий и справок используй wiki/chabadpedia, не заменяй ими первоисточники».
- Возможность сохранять полезные wiki-чанки в Qdrant с payload `source=...`.

## 8. Интеграция с Qdrant и Mem0
- Очередь ingest → worker, idempotent upsert, hashing `session_id|speaker|text`.
- Neo4j: факты, топики, контекст, связь с Qdrant (векторный поиск).
- Метрики: latency, cache hits, errors.

## 9. Обработка ошибок и защита
- Проверка заголовка `MediaWiki-API-Error`, ретраи (429/5xx).
- Соблюдение `maxlag`.
- Всегда указывать `User-Agent`.
- Circuit breaker для Mem0/Redis.

## 10. Улучшения UX
- Интерактивность: «найдено N источников», уточнения («расскажи больше про мнение Раши»).
- Режимы глубины: краткий, развернутый, академический.
- Персонализация: профили пользователей, предпочтения комментаторов, история запросов.

## 11. Отложенные улучшения (после подтверждения MVP)
- **Многоуровневая валидация и анти-галлюцинации.** Автоматическая проверка цитат, оценка уверенности, красные флажки для спорных утверждений.
- **Расширенная структурированная память.** Дополнительные коллекции в Qdrant, глубже интегрированный Mem0 и гибридные поисковые стратегии.
- **Эксперименты с эмбеддингами.** Многоязычные и специализированные модели для иврита/арамейского.
- **Интерактивность и персонализация.** Режимы глубины, профили пользователей, отображение прогресса.
- **Халахийная и этическая проверка.** Дополнительные фильтры и пометки консенсуса.
- **Расширенный Document Formatter & Export.** Полноценные шаблоны (LaTeX/DOCX), автоматические библиографии, продвинутый форматтер.
- **Расширенный мониторинг качества.** Метрики `citation_accuracy`, `source_coverage`, `response_coherence` и дашборды.

> Эти задачи запланированы после того, как базовый deepresearch и сохранение результатов в документ будут подтверждены.

## 11.1 Обновления сентября 2025
- 2025-09-17: Планировщик работает; следующая задача — протестировать на крупной модели после завершения текущего тестирования.
- 2025-09-17: Планировщик и нормализация ссылок впрямую работают на qwen3:8b; тесты проходят, далее будем проверять на более крупной модели.

- 2025-09-17: Планировщик и нормализация ref'ов доведены до рабочего состояния; deepresearch снова собирает источники. Осталось проверить англоязычные формулировки guiding questions/outline и при необходимости усилить промпт qwen3:8b.



- 2025-09-17: Автоматический планировщик не запускается из-за обращения к недоступной модели `gpt-4o-mini`; текущий процесс brain всё ещё держит старые переменные окружения, поэтому deepresearch временно пропускается. Требуется перезапустить сервисы после обновления `.env`.
## 11.2 План тестирования (сентябрь 2025)
1. Прогнать `run_research_test.py` с типовым запросом (Шулхан Арух / Берешит) и убедиться, что план → корпус → драш формируются без ошибок.
2. Добавить unit-тест для `_build_research_context_message`, проверяющий формат системного сообщения и включение outline/источников.
3. Замокать сторонний ответ и протестировать `_curate_links_with_llm` + fallback `_select_priority_links` (контракт на JSON, дедупликация категорий).
4. После автотестов — ручной прогон через CLI, сверить, что итоговая драш ссылается на выбранные источники и укладывается в `research_depth`.

- Добавлен планировщик `analyze_research_request`, который возвращает фокус, outline, канонические категории и параметр `research_depth`.
- Подготовка корпуса использует LLM-курацию `_curate_links_with_llm` и fallback `_select_priority_links` для отбора релевантных ссылок.
- Чанкование и экспорт метаданных фиксируют `research_depth` и подробные списки источников в `research_info`.
- Финальный промпт пополняется через `_build_research_context_message`, чтобы drasha следовала согласованному плану и ссылалась на собранный корпус.

## 12. TODO и контрольная панель

### 12.1 Глобальные вехи
- [ ] Поднять базовые эндпоинты `/sefaria/*` и убедиться, что тексты и ссылки возвращаются стабильно.
- [ ] Стабилизировать deepresearch-пайплайн (RAG + reasoning) и добиться воспроизводимого качества ответов.
- [x] Реализовать базовый экспорт результата (сохранение структуры ответа в JSON; см. brain/document_export.py).
- [ ] Наладить ingest в Qdrant и Mem0 с ключевыми метаданными для повторного использования.
- [ ] Собрать минимальный мониторинг (latency, ошибки) и чеклист ручной проверки.
- [ ] Прогнать e2e сценарий: запрос → deepresearch → сохраненный документ.

### 12.2 Breakdown по подсистемам
**Sefaria / RAG**
- [ ] Нормализация ссылок, aliased TOC, устойчивые вызовы `sefaria_get_text`/`sefaria_get_links`.
- [ ] Лимиты по категориям и комментаторам, приоритеты, базовая фильтрация.
- [ ] Кэширование ответов и fallback-сценарий при недоступности API.

**Память и Qdrant**
- [x] Очередь ingest → worker (через `/ltm/store`), ключевые метаданные, коллекции per-study.
- [ ] Синхронизация с Mem0 для вытягивания предыдущих фактов.
- [ ] Минимальный fallback (скролл/поиск по сырому тексту) при сбоях Qdrant.

**LLM и агент**
- [ ] Системный промт в стиле сихот, управление историей диалога.
- [ ] ReAct с контролем использования инструментов и защитой от циклов.
- [ ] Метрики использования инструментов и базовые circuit-breakers (без автоматической валидации).

**Документ и экспорт**
- [x] Сохранение результата в JSON (минимальный формат готов; Markdown впереди).
- [ ] Структура: введение, разделы анализа, вывод, блок источников.
- [ ] Подготовка API/функции для скачивания/архивации документа.

**TTS (минимальный режим)**
- [ ] Поддержка simple/streaming режимов и ручного включения.
- [ ] Разделение на главы и опция skip footnotes при необходимости.
- [ ] Проверка доступности TTS сервиса, без обязательной автоматизации.

**Тестирование и эксплуатация**
- [ ] Юнит-тесты ключевых компонентов (Sefaria utils, чанкование, экспорт).
- [ ] E2E сценарии: запрос → deepresearch → сохраненный документ.
- [ ] Минимальные метрики (ответы, ошибки, время).
- [ ] Документация деплоя, прогревов кэша, ручной проверки качества.

## 13. План реализации (roadmap)

### Спринт 1. Сефария — базовая интеграция
- [x] Поднять REST-эндпоинты `/sefaria/text`, `/sefaria/links` и обеспечить стабильные ответы.
- [ ] Настроить фильтрацию links (лимиты, приоритеты) и нормализацию `tref`.
- [ ] Добавить кэширование ответов и ручной чек-лист для проверки выборки источников.

### Спринт 2. Индексация и память
- [x] Реализовать чанкование (200–300 символов, overlap 30–50) и embeddings (пока используем OpenAI; fallback к Ollama подготовлен).
- [ ] Настроить ingest → worker в Qdrant с метаданными per-study и связкой с Mem0.
- [ ] Подготовить минимальный fallback-поиск при сбоях Qdrant.

### Спринт 3. Агент и deepresearch
- [ ] Связать инструменты `sefaria_get_*`, `recall_long_term_memory`, `vector.search`, wiki/chabadpedia в ReAct-петле.
- [ ] Обновить системный промт (стиль сихот) и контроль цикла вызовов инструментов.
- [ ] Протестировать несколько запросов и зафиксировать чек-лист качества ответов.

### Спринт 4. Сохранение результата
- [ ] Сформировать структуру ответа (введение, анализ, вывод, источники) в виде Python-объекта.
- [x] Реализовать базовый экспорт в JSON (автоэкспорт для `chevruta_deepresearch`).
- [ ] Добавить ручной сценарий: запрос → сохраненный файл → проверка контента.

### Спринт 5. Наблюдаемость и ручной QA
- [ ] Собрать минимальные метрики (время ответа, ошибки инструментов, количество источников).
- [ ] Настроить логирование прогресса ("ищу комментарии... найдено N источников...").
- [ ] Подготовить e2e-тест: запрос → deepresearch → сохраненный документ → (опционально) TTS.

### Спринт 6. Отложенные улучшения (см. раздел 11)
- [ ] Вернуться к валидации цитат, расширенной памяти, продвинутому форматтеру.
- [ ] Добавить расширенный мониторинг качества и автоматизацию проверок.
- [ ] Рассмотреть полноценную автоматическую озвучку после подтверждения стабильности deepresearch.

## 14. Перспектива после DeepResearch
- Главная цель текущего этапа — добиться стабильного deepresearch: уверенная выдача источников, связный анализ и сохранение результата в документ.
- После подтверждения стабильности возвращаемся к расширенному Document Formatter & Export (PDF/DOCX, автоматическая верстка, библиографии).
- Полноценная автоматическая озвучка длинных драш останется ручной задачей до завершения deepresearch; затем рассмотрим формат аудиокниги.

## 15. Обновления (сентябрь 2025)
- **Sefaria related-links.** `sefaria_get_links` теперь использует только `/api/related` (секцию `links`), фильтрует и логирует объём данных, чтобы избежать расхождений со списками из `/links` и сократить дубли.
- **Квоты и сводки по источникам.** `_collect_commentaries` применяет квоты по категориям, ограничивает число чанков (8 для первичных текстов, 4 для комментариев), строит сводки (primary/supporting/commentary buckets) и передаёт их в системный промпт, а инструменты памяти возвращают компактные сниппеты.
- **Research memory preview.** После инжеста исследовательской коллекции запрос к `/research/recall` автоматически собирает структурированный preview, который добавляется в системный контекст LLM.
- **Mem0 на Ollama.** Конфиг памяти поддерживает Ollama для эмбеддингов и LLM одновременно; OpenAI/OpenRouter становятся необязательными, ошибки квоты обрабатываются мягко.

## 16. План по reasoning (Q4 2025)
- **Слой заметок.** После загрузки каждого чанка формируем «конспект» (сторинги, вопросы, вывод) и сохраняем в отдельную коллекцию `..._notes`, чтобы итоговая модель опиралась на осмысленные выдержки, а не только на сырой текст.
- **Многошаговый ответ.** Вводим этап черновика: LLM №1 готовит черновой обзор по заметкам, LLM №2 оформляет финальный текст; черновик и reasoning фиксируются в памяти и могут быть просмотрены.
- **Прозрачные логи.** Добавляем сохранение `<think>` или эквивалентных reasoning-записей в лог/preview, чтобы «цепочка рассуждений» была доступна для отладки, но при необходимости исключалась из финального ответа.
- **Расширенный RAG.** Для глубины исследования LLM может выполнять точечные запросы к памяти (по конкретным refs), увеличивая лимиты и контролируя, что все используемые источники указаны в финальном списке.

## 17. Обновления архитектуры (Сентябрь 2025)

В результате совместной работы был проведен значительный рефакторинг и исправлен ряд проблем в пайплайне `deepresearch`.

### 17.1 Исправление ошибок

- **Устранена проблема с планировщиком:** Изначально планировщик не работал из-за неверно указанной модели и серии ошибок импорта в `research_planner.py`. Все ошибки были исправлены, а модель заменена на рабочую (`qwen3:8b`).
- **Исправлена работа Куратора:** Куратор ссылок падал на больших запросах из-за переполнения контекстного окна. Внедрен механизм предварительной фильтрации, который сначала отбирает кандидатов по приоритетным комментаторам и жесткому лимиту (`ASTRA_CURATOR_MAX_CANDIDATES`), и только потом передает их LLM.

### 17.2 Новая архитектура "Data-First"

Основное изменение — переход от "гадания" к работе с реальными данными. Старый пайплайн пытался угадать релевантные источники. Новый работает по детерминированному, многошаговому процессу:

1.  **Умный парсинг запроса (`parse_initial_request`):**
    *   Вместо простого извлечения ссылки, новый парсер на базе LLM анализирует запрос пользователя и извлекает из него:
        *   Основную ссылку (`primary_ref`).
        *   Желаемые категории (`categories`).
        *   Приоритетных комментаторов (`priority_commentators`).
        *   Глубину исследования (`search_depth`).
        *   **Цель исследования (`research_goal`)** — например, "найти практический урок" или "сравнить мнения".

2.  **Сбор и Курирование:**
    *   Система получает из Сефарии большой список реальных связанных ссылок.
    *   **Куратор (LLM)**, используя `research_goal` и `priority_commentators`, отбирает из них наиболее подходящие.

3.  **Планирование по данным:**
    *   **Планировщик (`analyze_research_request`)** теперь не угадывает источники, а получает готовый качественный список от Куратора. Его задача — составить по этому списку и `research_goal` детальный план: `focus`, `guiding_questions`, `outline`.

4.  **Обогащение: Конвейер "Чанк → Заметка":**
    *   Это ключевое улучшение. Система теперь не просто хранит сырой текст. Для **каждого** фрагмента (чанка) каждого источника вызывается LLM, который создает краткую осмысленную "заметку".
    *   Эти заметки сохраняются в отдельную коллекцию `..._notes`, формируя слой структурированных, проанализированных данных.

5.  **Создание черновика (`_generate_research_draft`):**
    *   Активирована ранее неиспользуемая функция. После сбора всех "заметок" отдельная LLM пишет по ним предварительный черновик (`draft`).

6.  **Финальный синтез:**
    *   Системный промпт в `personalities.json` был полностью переписан. Новая роль финальной модели — не "исследователь", а "писатель-аналитик".
    *   Она получает в контекст все подготовленные данные: план, цель, все заметки и **полный текст черновика**.
    *   Ее задача — на основе всего этого написать финальный, качественный и структурированный текст.

7.  **Единая коллекция на исследование:**
    *   Исправлена логика хранения. Теперь все чанки и заметки для одного исследования хранятся в **одной** общей коллекции (например, `chevruta_deepresearch_shulchan_arukh_...`), что делает финальный поиск по памяти (`recall_research_sources`) надежным и эффективным.

## 18. План по улучшению Reasoning-двигателя (Сентябрь 2025)

Хотя текущий пайплайн работает, он все еще является относительно простым. Ниже описан план по внедрению полноценного "reasoning-двигателя" для более глубокого и управляемого исследования.

### Проблемы текущего подхода

- **Жесткий лимит итераций:** В `get_llm_response_stream` стоит жесткий предел в 3 итерации, чего часто недостаточно для сложной драши.
- **Однократный план:** Куратор и планировщик отрабатывают один раз, без возможности перепланирования, если контекста не хватило.
- **Отсутствие явной стратегии:** Нет четкой стратегии "план → добыча → проверка покрытия → до-добыча → черновик → самокритика → правка".

### Конкретные улучшения

#### 1. Эластичные итерации инструментов [РЕАЛИЗОВАНО]

Сделать количество итераций зависимым от сложности задачи.

- **Где править:** `main.py` (`get_llm_response_stream`)
- **Логика:**
  ```python
  # вместо фиксированного: max_iters = 3
  depth = 10
  if isinstance(plan, dict):
      d = plan.get("research_depth")
      if isinstance(d, int) and d > 0:
          depth = d
  max_iters = max(4, min(12, 3 + (depth + 4)//5))
  ```
- **Дополнительно:** Расширить механизм защиты от циклов до хранения последних 3 наборов tool-calls.

#### 2. Явное "размышление" в промпте агента

Добавить в системный промпт финального агента инструкцию думать в тегах `<think>` и самому проверять полноту исследования.

- **Где править:** `main.py` (рядом с `_build_research_context_message`)
- **Пример инструкции:**
  > "Внутренние размышления пиши внутри <think>…</think>, не повторяй их пользователю."
  > "Перед финальным ответом убедись, что покрыты: [primary], Commentary, Midrash, (по задаче: Halakhah/Responsa/Chasidut/Kabbalah/Jewish Thought). Если чего-то нет — вызови инструмент и добери источники."

#### 3. "Scratchpad" для рассуждений [ЧАСТИЧНО РЕАЛИЗОВАНО]

- Реализован основной механизм "Чанк -> Заметка" и коллекция `..._notes`. Агент может делать по ней поиск.

Создать отдельную коллекцию в Qdrant (`..._scratch_collection`) для промежуточных мыслей и гипотез агента.

- **Где править:** `research.py`, `main.py`
- **Логика:** Агент сможет делать RAG-запросы к своим же собственным мыслям (`recall_research_sources` с коллекцией scratchpad), чтобы проверять, "что уже решено", перед тем как делать новые запросы к Сефарии.

#### 4. Проверка покрытия ("Coverage Check") и перепланирование [РЕАЛИЗОВАНО]

После первичного сбора данных автоматически проверять, все ли запрошенные категории и комментаторы были найдены.

- **Где править:** `research.py` (в `prepare_deepresearch_payload`)
- **Логика:**
  ```python
  missing = _compute_missing_coverage(filtered_commentaries, requested_categories)
  if missing:
      # пробуем relax
      relaxed = await _collect_commentaries(tref, categories + missing, max_commentaries+2, ...)
      # ...
  ```

#### 5. Куратор с "самокритикой" (Reflection)

Добавить второй проход для куратора:
1.  Генерируется черновик (`_generate_research_draft`).
2.  Запускается "CRITIC"-модель с инструкцией: "проверь, каких источников не хватает".
3.  Если есть пробелы, запускается дополнительный сбор данных.

- **Где править:** `main.py`

#### 6. Self-Consistency для плана [ЗАМЕНЕНО]

- Вместо этого реализован более мощный Map-Reduce механизм для планировщика, решающий ту же задачу более надежно.

Заставить планировщика генерировать 3 варианта плана (`outline`) и выбирать лучший путем "голосования" (повторяющиеся пункты).

- **Где править:** `research_planner.py` (в `analyze_research_request`)
- **Логика:** Сделать 3 параллельных вызова `_call_llm` с `temperature≈0.8` и объединить результаты.

#### 7. Параллелизм и разумные лимиты [ОТЛОЖЕНО]

- Ускорить загрузку текстов комментаторов через `asyncio.Semaphore`.
- Усилить предварительную фильтрацию ссылок перед передачей куратору.
- **Где править:** `research.py`

#### 8. Внедрение ReAct-стиля

Четко прописать в системном промпте цикл "Думай → Докажи инструментом → Продолжай".

- **Где править:** `main.py` (системный промпт)
- **Паттерн:**
  1.  `<think>`Какие пробелы?`<think>`
  2.  `tool_call()`
  3.  `<think>`Что нового? Чего не хватает?`<think>`
  4.  Повторять, пока покрытие не достигнуто.

#### 9. Двухмодельный режим: Reasoner + Writer

Разделить роли:
- **Reasoner (дешевая модель):** Занимается итеративным сбором данных и составлением плана.
- **Writer (дорогая модель):** Берет готовые заметки и пишет финальный текст.
- **Где править:** `main.py`

### Готовые вставки в системный промпт

**В начало system (чат-агента):**
```
Ты — исследователь и главный редактор драши. 
Всегда размышляй во внутренних блоках <think>…</think> (они не показываются пользователю).
Работай по циклу:
1) <think>Какие аспекты и категории нужно покрыть? Какие источники нужны?</think>
2) Вызови нужные инструменты (Сефария тексты, links, память исследования).
3) <think>Что нового открылось? Чего ещё не хватает для полноценной сихи?</think>
Повторяй 1–3, пока не достигнуто покрытие:
- есть первичный текст,
- есть мнения из Commentary + Midrash,
- при необходимости (по задаче): Halakhah/Responsa или Chasidut/Kabbalah или Jewish Thought.
Если чего-то не хватает — продолжай вызывать инструменты.
Никогда не выдумывай источники: для каждой цитаты должен быть реальный ref.
```

**Ближе к концу:**
```
Перед финальным ответом:
- Сверь список покрытых категорий и приоритетных комментаторов с задачей.
- Если есть пробелы — добери источники.
- Затем выдай финальную сиха-структуру и примечания.
```

## 20. Итог и Финальная Архитектура (Сентябрь 2025)

После длительного тестирования и множества итераций была выработана финальная, наиболее надежная и умная архитектура для `deepresearch` пайплайна.

Ключевым решением стал **полный отказ от использования LLM на этапе курирования (отбора) источников** из-за его нестабильной работы со структурированными данными (JSON).

**Финальный пайплайн выглядит так:**

1.  **Умный парсинг:** LLM анализирует запрос пользователя, извлекая цель, основной источник, категории, приоритетных комментаторов и глубину.
2.  **Поиск по категориям:** Система итеративно для каждой запрошенной категории запрашивает у Сефарии большой список связанных ссылок.
3.  **Детерминированное курирование:** Вместо LLM, быстрый и надежный Python-алгоритм (`select_priority_links`) отбирает из этого списка нужное количество лучших ссылок, основываясь на приоритетах пользователя.
4.  **Планирование (Map-Reduce):** Отфильтрованный и качественный список ссылок передается планировщику, который по принципу "Разделяй и властвуй" строит по ним детальный план.
5.  **Обогащение ("Чанк -> Заметка"):** Для каждого источника из плана каждый его фрагмент осмысляется LLM и превращается в краткую "заметку", которая сохраняется в базу знаний (`..._notes`).
6.  **Создание черновика:** LLM пишет черновик на основе всех созданных "заметок".
7.  **Финальный синтез (ReAct):** Финальная модель-редактор получает весь контекст (план, заметки, черновик) и в цикле "Думай-Проверяй-Действуй" обращается к созданной базе знаний (`recall_research_sources`), чтобы уточнить детали, после чего пишет итоговый текст.

Этот подход сочетает интеллект LLM на задачах, где он силен (анализ запроса, планирование, осмысление, написание), с надежностью и скоростью детерминированного кода на критически важном этапе отбора данных.





## Итоги отладки (19 Сентября 2025)

В результате интенсивной совместной отладки была полностью восстановлена и стабилизирована работа всего пайплайна голосового ассистента. Был исправлен целый ряд критических ошибок, блокировавших выполнение исследования и работу с памятью.

### Ключевые решенные проблемы:

1.  **Проблема `status=skipped` (пропуск исследования):**
    - **Причина:** Обнаружено две логические ошибки в `brain/research.py`. Во-первых, код искал в плане исследования ключ `primary_refs` (мн. число), тогда как планировщик создавал `primary_ref` (ед. число). Во-вторых, категория `Commentary` не была включена в список категорий, для которых нужно искать связанные источники.
    - **Решение:** Код был исправлен для поддержки обоих вариантов ключа (`primary_ref` и `primary_refs`) и в список поисковых категорий был добавлен `Commentary`.

2.  **Каскадная ошибка конфигурации сервиса `memory`:**
    - **Причина:** Установлено, что сервис `memory` имеет собственную архитектуру конфигурации и ищет локальный файл `.env` в своей директории (`memory/.env`), игнорируя корневой `.env` файл. Из-за отсутствия этого файла сервис использовал неверные настройки по умолчанию, что приводило к серии разнообразных ошибок: от `embeddinggemma not found` и `AttributeError` до `NameError` при запуске.
    - **Решение:** Был создан корректный файл `memory/.env`, который указывает сервису использовать локальный сервер Ollama. Также был исправлен `memory/config.py` для корректной загрузки конфигурации и `k_graph.py` для правильной инициализации клиента.

3.  **Сбой `mem0` из-за сетевого доступа:**
    - **Причина:** Библиотека `mem0` при инициализации пыталась автоматически скачать модель с официального репозитория Ollama, но сетевые настройки среды блокировали доступ к `registry.ollama.ai`, что приводило к падению сервиса `memory`.
    - **Решение:** Код библиотеки `mem0` внутри виртуального окружения (`mem0/embeddings/ollama.py`) был модифицирован — мы отключили автоматическую загрузку, добавив `pass` в соответствующий блок кода.

4.  **Ошибка `Connection Error` (таймаут клиента):**
    - **Причина:** Процесс исследования и осмысления источников занимал несколько минут, в течение которых `brain` не отправлял никаких данных клиенту. Это приводило к разрыву соединения по таймауту.
    - **Решение:** В скрипт запуска `start_cli_old.py` была добавлена настройка `--timeout-keep-alive 600` для сервиса `brain`, увеличившая время ожидания до 10 минут.

### Финальная рабочая архитектура:

- **Единая точка конфигурации:** Основные настройки моделей задаются в корневом файле `.env`. 
- **Изолированная конфигурация `memory`:** Сервис `memory` использует собственный файл `memory/.env` для указания провайдеров LLM и эмбеддингов, что обеспечивает его независимую и корректную работу.
- **Локальная обработка:** Вся обработка (LLM-задачи, эмбеддинги) полностью переведена на локальный сервер **Ollama**, что исключает зависимость от платных внешних API и решает проблему с нехваткой кредитов.
- **Стабильность:** Благодаря исправлению всех вышеперечисленных проблем, система теперь стабильно выполняет полный цикл: получает запрос, планирует исследование, собирает и осмысляет источники, сохраняет их в память и готова к генерации финального ответа.